\documentclass[a4paper,12pt]{article}
\usepackage{ amssymb }
\usepackage{mathtools}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{listings}
\usepackage{color}
\usepackage[utf8]{inputenc}
\usepackage{amsfonts}
 \usepackage{tikz}
 \usepgflibrary{fpu}
 \usepackage{graphicx}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

\begin{document}

\noindent{
\framebox{
\begin{minipage}{\dimexpr\textwidth-2\fboxsep-2\fboxrule\relax}
\vspace{2mm}
1.  Let $f(n)$ and $g(n)$ be asymptotically nonnegative functions. Using the basic definitions of $\Theta$-notation, prove that $\max(f(n), g(n)) = \Theta(f(n) + g(n))$.
\vspace{2mm}
\end{minipage}
}
}

For simplicity, let us define $h(n) = \max(f(n),g(n))$. To prove $h(n) = \Theta(f(n) + g(n))$, we must find positive constants $c_1$, $c_2$ and $n_0$ such that $0 \le c_1\cdot \{f(n) + g(n)\} \le h(n) \le c_2 \cdot \{f(n) + g(n)\}$ for all $n \ge n_0$.

Both $f(n)$ and $g(n)$ are asymptotically nonnegative functions, so that for some $n'$ we have $0 \le f(n)$ and $0 \le g(n)$ for all $n \ge n'$. Select $n_0 = n'$.

Suppose further without loss of generality that $g(n) \le f(n) = h(n)$. Then $f(n) + g(n) \le 2 \cdot h(n)$, so $\frac{1}{2} \cdot \left(f(n) + g(n)\right) \le h(n)$ and we can choose $c_1 = 1/2$. Likewise since both $f(n)$ and $g(n)$ are asymptotically nonnegative, we can write $f(n) \le f(n) + g(n)$ for $n \ge n_0 = n'$ and choose $c_2=1$.

Hence with $c_1 = 1/2$, $c_2 = 1$ and $n_0 = n'$, we have $\max(f(n), g(n)) = \Theta(f(n) + g(n))$.

\vspace{2mm}
\noindent{
\framebox{
\begin{minipage}{\dimexpr\textwidth-2\fboxsep-2\fboxrule\relax}
\vspace{2mm}
2.  Show that for any real constants $a$ and $b$, where $b > 0$,

\[ (n+a)^b = \Theta(n^b) \]

\vspace{1mm}
\end{minipage}
}
}

To show that $(n+a)^b = \Theta(n^b)$, we must find positive constants $c_1$, $c_2$, and $n_0$ such that for all $n \ge n_0$, we have:

\[ c_1 n^b \le (n+a)^b \le c_2 n^b \]

\[ c_1 n^b \le \{n(1+\frac{a}{n})\}^b \le c_2 n^b \]

\[ c_1 n^b \le  n^b\left(1 + \frac{a}{n}\right)^b \le c_2 n^b \]

\[ c_1 \le \left(1 + \frac{a}{n}\right)^b \le c_2 \]

Select $n_0 = 2|a|$, and suppose that $a < 0$. Then the middle term becomes $\left(1 + a/n_0\right)^b$ $\rightarrow$ $\left(\frac{1}{2}\right)^b$ when $n=n_0$ and increases as $n \ge n_0$, which suggests that we should choose $c_1 = \left(\frac{1}{2}\right)^b$. Likewise, suppose instead that we have $a > 0$. Then the middle term becomes $\left(1 + a/n_0\right)^b$ $\rightarrow$ $\left(\frac{3}{2}\right)^b$ when $n=n_0$ and decreases as $n \ge n_0$, which suggests that we should choose $c_2 = \left(\frac{3}{2}\right)^b$.

Hence with $c_1 = \left(\frac{1}{2}\right)^b$, $c_2 = \left(\frac{3}{2}\right)^b$ and $n_0 = 2|a|$, we have $(n+a)^b = \Theta(n^b)$.

\vspace{2mm}
\noindent{
\framebox{
\begin{minipage}{\dimexpr\textwidth-2\fboxsep-2\fboxrule\relax}
\vspace{2mm}
3.  Explain why the statement ``The running time of algorithm $A$ is at least $O(n^2)$" is content-free.
\vspace{2mm}
\end{minipage}
}
}

The statement doesn't make sense, or is meaningless, because $O(n^2)$ is an \textit{upper} bound on the running time of $A$. It could make sense to that the running time of $A$ is at \textit{most} $O(n^2)$, or it could also make sense to say that the running time of $A$ is at \textit{best} $\Omega(n^2)$, but the statement as it stands doesn't make sense in the context of what $O(n^2)$ means. 

\vspace{2mm}
\noindent{
\framebox{
\begin{minipage}{\dimexpr\textwidth-2\fboxsep-2\fboxrule\relax}
\vspace{2mm}
4.  Is $2^{n+1} = O(2^n)$? Is $2^{2n} = O(2^n)$?
\vspace{2mm}
\end{minipage}
}
}

To determine if $2^{n+1} = O(2^n)$, we must find positive constants $c$ and $n_0$ such that $2^{n+1} \le c \cdot 2^n$ for all $n \ge n_0$. Dividing both sides of this inequality by $2^n$, we obtain $2 \le c$. Select $n_0 = 1$ and $c = 3$, and we have established that $2^{n+1} = O(2^n)$.

To determine if $2^{2n} = O(2^n)$, we must find positive constants $c$ and $n_0$ such that $2^{2n} \le c \cdot 2^n$ for all $n \ge n_0$. Dividing both sides of this inequality by $2^n$, we obtain $2^n \le c$. Clearly, there is no choice of $c$ that will satisfy this relation. Hence,  we conclude that $2^{2n} \not= O(2^n)$.

\vspace{2mm}
\noindent{
\framebox{
\begin{minipage}{\dimexpr\textwidth-2\fboxsep-2\fboxrule\relax}
\vspace{2mm}
5.  Prove Theorem 2.1.
\vspace{2mm}
\end{minipage}
}
}

Theorem 2.1 states that for any two functions $f(n)$ and $g(n)$, $f(n) = \Theta(g(n))$ if and only if $f(n) = O(g(n))$ and $f(n) = \Omega(g(n))$.

Suppose that $f(n) = \Theta(g(n))$. This means that we can find positive constants $c_1$, $c_2$ and $n_0$ such that $0 \le c_1g(n) \le f(n) \le c_2 g(n)$ for all $n \ge n_0$. Clearly, this means that we have found positive constants $c_2$ and $n_0$ such that $0 \le f(n) \le c_2g(n)$ for all $n \ge n_0$, hence $f(n) = O(g(n))$. Likewise, it also means that we have found positive constants $c_1$ and $n_0$ such that $0 \le c_1g(n) \le f(n)$ for all $n \ge n_0$, hence $f(n) = \Omega(g(n))$.

This establishes $f(n) = \Theta(g(n)) \Rightarrow f(n) = O(g(n)) \land f(n) = \Omega(g(n))$.

Suppose next that $f(n) = O(g(n))$ and $f(n) = \Omega(g(n))$. Since $f(n) = \Omega(g(n))$, we can find positive constants $c_1$ and $n_1$ such that $0 \le c_1g(n) \le f(n)$ for all $n \ge n_1$. Likewise, since $f(n) = O(g(n))$, we can find positive constants $c_2$ and $n_2$ such that $0 \le f(n) \le c_2g(n)$ for all $n \ge n_2$. Let $n_0 = \max(n_1, n_2)$. We have now identified positive constants $c_1$, $c_2$ and $n_0$ such that $0 \le c_1g(n) \le f(n) \le c_2g(n)$ for all $n \ge n_0$. Hence, $f(n) = \Theta(g(n))$.

This establishes $f(n) = O(g(n)) \land f(n) = \Omega(g(n)) \Rightarrow f(n) = \Theta(g(n))$.

Hence $f(n) = \Theta(g(n)) \iff f(n) = O(g(n)) \land f(n) = \Omega(g(n))$.

\vspace{2mm}
\noindent{
\framebox{
\begin{minipage}{\dimexpr\textwidth-2\fboxsep-2\fboxrule\relax}
\vspace{2mm}
6.  Prove that the running time of an algorithm is $\Theta(g(n))$ if and only if its worst-case running time is $O(g(n))$ and its best-case running time is $\Omega(g(n))$.
\vspace{2mm}
\end{minipage}
}
}

Let $T(n)$ be the running time of the algorithm and suppose that $T(n) = \Theta(g(n))$. Hence we can find positive numbers $c_1$, $c_2$ and $n_0$ such that $0 \le c_1 \cdot g(n) \le T(n) \le c_2 \cdot g(n)$ for all $n \ge n_0$. The longest $T(n)$ could possibly take to complete is bounded above by $c_2 \cdot g(n)$, hence $T(n) = O(g(n))$. The fastest that $T(n)$ could possibly take to complete is bounded below by $c_1 \cdot g(n)$, hence $T(n) = \Omega(g(n))$.

Suppose that the fastest running time for $T(n)$ is bounded below by $\Omega(g(n))$. This means that we can find positive constants $c_1$ and $n_1$ such that $0 \le c_1\cdot g(n) \le f(n)$ for all $n \ge n_1$. Suppose further that the worst-case running time for $T(n)$ is bounded above by $O(g(n))$. This means that we can find positive constants $c_2$ and $n_2$ such that $0 \le f(n) \le c_2 \cdot g(n)$ for all $n \ge n_2$. Let $n_0 = \max(n_1, n_2)$. Therefore, we have $0 \le c_1 \cdot g(n) \le f(n) \le c_2 \cdot g(n)$ for all $n \ge n_0$ and hence $T(n) = \Theta(g(n))$.

\vspace{2mm}
\noindent{
\framebox{
\begin{minipage}{\dimexpr\textwidth-2\fboxsep-2\fboxrule\relax}
\vspace{2mm}
7.  Prove that $o(g(n)) \cap \omega(g(n))$ is the empty set. 
\vspace{2mm}
\end{minipage}
}
}

The notations $o(g(n))$ and $\omega(g(n))$ are used to indicate bounds that are \textit{not} asymptotically tight. That is, we have $f(n) = o(g(n))$ if for \textit{any} positive constant c, we can find a positive constant $n_0$ such that $0 \le f(n) < cg(n)$ for all $n \ge n_0$. Likewise, we have $f(n) = \omega(g(n))$ if for \textit{any} positive constant c, we can find a positive constant $n_0$ such that $0 \le cg(n) < f(n)$ for all $n \ge n_0$.

Suppose we have $f(n) = \omega(g(n))$, and select an arbitrary positive constant $c_0 > 0$. Since $f(n) = \omega(g(n))$, we can find a positive constant $n_1 > 0$ such that $0 \le c_0\cdot g(n) < f(n)$ for all $n \ge n_1$. Suppose further that $f(n) = o(g(n))$, so that we can find another positive constant $n_2 > 0$ such that $0 \le f(n) < c_0 \cdot g(n)$ for all $n \ge n_2$. Let $n_0 = \max(n_1, n_2)$. It follows that we must have $c_0 \cdot g(n) < f(n) < c_0 \cdot g(n)$ for all $n \ge n_0$. This is a contradiction, and so $f(n)$ cannot be in both $\omega(g(n))$ and $o(g(n))$ simultaneously. Since the choice of $f(n)$ was arbitrary, we have $o(g(n)) \cap \omega(g(n)) = \emptyset$.

A second approach would be to use a result given in the text, namely that $f(n) = \omega(g(n))$ if and only if $g(n) = o(f(n))$. Suppose that $\omega(g(n)) \cap o(g(n)) \not= \emptyset$ and choose some $f(n) \in \omega(g(n)) \cap o(g(n))$. Then $f(n) = \omega(g(n))$ and $f(n) = o(g(n))$. But since $f(n) = \omega(g(n))$, we must have that $g(n) = o(f(n))$ from the result in the text. 

Since $f(n) = o(g(n))$, we can find a $n_1 > 0$ such that $0 \le f(n) < c \cdot g(n)$ for any $c > 0$ and all $n \ge n_1$. Likewise, since $g(n) = o(f(n))$, we can find $n_2 > 0$ such that $0 \le g(n) < c \cdot f(n)$ for any $c > 0$ and all $n \ge n_2$. Choose $n_0 = \max(n_1, n_2)$. It follows then that we have $f(n) < c\cdot g(n)$ and $g(n) < c\cdot f(n)$ for any $c > 0$ and all $n \ge n_0$, which is a contradiction. Hence, no such $f(n)$ exists and $o(g(n)) \cap \omega(g(n)) = \emptyset$.

\vspace{2mm}
\noindent{
\framebox{
\begin{minipage}{\dimexpr\textwidth-2\fboxsep-2\fboxrule\relax}
\vspace{2mm}
8.  We can extend our notation to the case of two parameters $n$ and $m$ that can go to infinity independently at different rates. For a given function $g(n,m)$, we denote by $O(g(n,m))$ the set of functions

\vspace{5mm}

$O(g(n,m)) = \{f(n,m) :$ there exist positive constants $c$, $n_0$ and $m_0$ such that $0 \le f(n,m) \le c \cdot g(n,m)$ for all $n \ge n_0$ and $m \ge m_0 \}$

\vspace{5mm} 

Give corresponding definitions for $\Omega(g(n,m))$ and $\Theta(g(n,m))$.
\vspace{2mm}
\end{minipage}
}
}

$\Omega(g(n,m)) = \{ f(n,m) : $ there exist positive constants $c$, $n_0$ and $m_0$ such that $0 \le c \cdot g(n,m) \le f(n,m)$ for all $n \ge n_0$ and $m \ge m_0$ $\}$.

$\Theta(g(n,m)) = \{ f(n,m) : $ there exist positive constants $c_1$, $c_2$, $n_0$ and $m_0$ such that $0 \le c_1 \cdot g(n,m) \le f(n,m) \le c_2 \cdot g(n,m)$ for all $n \ge n_0$ and $m \ge m_0$$\}$.

\end{document}